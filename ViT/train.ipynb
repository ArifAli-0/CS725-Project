{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, auc, roc_curve, confusion_matrix\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import import_ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from vit.ipynb\n"
     ]
    }
   ],
   "source": [
    "from vit import ViT\n",
    "from vit import training_dataloader, validation_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "ViT (ViT)                                                    [32, 3, 224, 224]    [32, 2]              --                   True\n",
       "├─PatchEmbeddingLayer (patch_embedding_layer)                [32, 3, 224, 224]    [32, 197, 768]       175,872              True\n",
       "│    └─Conv2d (conv_layer)                                   [32, 3, 224, 224]    [32, 768, 14, 14]    590,592              True\n",
       "│    └─Flatten (flatten_layer)                               [32, 14, 14, 768]    [32, 196, 768]       --                   --\n",
       "├─Sequential (transformer_encoder)                           [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    └─TransformerBlock (0)                                  [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiHeadSelfAttentionBlock (msa_block)          [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
       "│    │    └─MultiLayerPerceptronBlock (mlp_block)            [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─TransformerBlock (1)                                  [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiHeadSelfAttentionBlock (msa_block)          [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
       "│    │    └─MultiLayerPerceptronBlock (mlp_block)            [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─TransformerBlock (2)                                  [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiHeadSelfAttentionBlock (msa_block)          [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
       "│    │    └─MultiLayerPerceptronBlock (mlp_block)            [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─TransformerBlock (3)                                  [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiHeadSelfAttentionBlock (msa_block)          [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
       "│    │    └─MultiLayerPerceptronBlock (mlp_block)            [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─TransformerBlock (4)                                  [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiHeadSelfAttentionBlock (msa_block)          [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
       "│    │    └─MultiLayerPerceptronBlock (mlp_block)            [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─TransformerBlock (5)                                  [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiHeadSelfAttentionBlock (msa_block)          [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
       "│    │    └─MultiLayerPerceptronBlock (mlp_block)            [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "├─Sequential (classifer)                                     [32, 768]            [32, 2]              --                   True\n",
       "│    └─LayerNorm (0)                                         [32, 768]            [32, 768]            1,536                True\n",
       "│    └─Linear (1)                                            [32, 768]            [32, 2]              1,538                True\n",
       "============================================================================================================================================\n",
       "Total params: 43,296,770\n",
       "Trainable params: 43,296,770\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 4.61\n",
       "============================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 1665.47\n",
       "Params size (MB): 115.79\n",
       "Estimated Total Size (MB): 1800.52\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 32\n",
    "writer = SummaryWriter(\"logs\")\n",
    "model = ViT()\n",
    "if os.path.exists(\"predict.pth\"):\n",
    "    print(\"Loading saved model\")\n",
    "    model.load_state_dict(torch.load(\"predict.pth\"))\n",
    "else:\n",
    "    print(\"Saved model weights not found\")\n",
    "optimizer = SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "summary(\n",
    "    model=model,\n",
    "    input_size=(\n",
    "        BATCH_SIZE,\n",
    "        3,\n",
    "        224,\n",
    "        224,\n",
    "    ),  # (batch_size, num_patches, embedding_dimension)\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(training_dataloader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        predicted_labels = torch.argmax(outputs, dim=1)\n",
    "        accuracy = accuracy_score(labels, predicted_labels)\n",
    "        # report every 100 batches\n",
    "        if i % 100 == 99:\n",
    "            print(\n",
    "                f\"Batch {i+1}, Loss: {running_loss / 100:.3f}, Training Accuracy: {accuracy}%\"\n",
    "            )\n",
    "            last_loss = running_loss / 100\n",
    "            tb_x = epoch_index * len(training_dataloader) + i + 1\n",
    "            tb_writer.add_scalar(\"Training loss\", last_loss, tb_x)\n",
    "            tb_writer.add_scalar(\"Training Accuracy\", accuracy, tb_x)\n",
    "            tb_writer.flush()\n",
    "            running_loss = 0.0\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "best_vloss = 1_000_000\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/arif/FML/FML_Project/train.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arif/FML/FML_Project/train.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEPOCH \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arif/FML/FML_Project/train.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39mtrain(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/arif/FML/FML_Project/train.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m avg_loss \u001b[39m=\u001b[39m train_one_epoch(epoch, writer)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arif/FML/FML_Project/train.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m running_vloss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arif/FML/FML_Project/train.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "\u001b[1;32m/home/arif/FML/FML_Project/train.ipynb Cell 7\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arif/FML/FML_Project/train.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m inputs, labels \u001b[39m=\u001b[39m data\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arif/FML/FML_Project/train.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/arif/FML/FML_Project/train.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arif/FML/FML_Project/train.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arif/FML/FML_Project/train.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/FML/FML_Project/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/FML/FML_Project/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m<string>:39\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n",
      "File \u001b[0;32m~/FML/FML_Project/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/FML/FML_Project/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m<string>:31\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n",
      "File \u001b[0;32m~/FML/FML_Project/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/FML/FML_Project/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/FML/FML_Project/env/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/FML/FML_Project/env/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"EPOCH {}:\".format(epoch + 1))\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch, writer)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_dataloader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "            predicted_labels = torch.argmax(voutputs, dim=1)\n",
    "\n",
    "            precision = precision_score(vlabels, predicted_labels, average=\"macro\")\n",
    "            recall = recall_score(vlabels, predicted_labels, average=\"macro\")\n",
    "            f1score = f1_score(vlabels, predicted_labels, average=\"macro\")\n",
    "\n",
    "            accuracy = accuracy_score(vlabels, predicted_labels)\n",
    "\n",
    "            if i % 100 == 99:\n",
    "              print(\"Predicted labels:\", predicted_labels)\n",
    "              print(\"Actual labels:\", vlabels)\n",
    "              print(\"Validation Accuracy\", accuracy)\n",
    "              print(\"Validation Precision\", precision)\n",
    "              print(\"Validation Recall\", recall)\n",
    "              print(\"Validation F1 score\", f1score)\n",
    "            writer.add_scalar(\"Validation Precision\", precision, epoch)\n",
    "            writer.add_scalar(\"Validation Recall\", recall, epoch)\n",
    "            writer.add_scalar(\"Validation F1-score\", f1score, epoch)\n",
    "            writer.add_scalar(\"Validation Accuracy\", accuracy, epoch)\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print(\"LOSS train {} valid {}\".format(avg_loss, avg_vloss))\n",
    "\n",
    "    writer.add_scalars(\n",
    "        \"Training vs. Validation Loss\",\n",
    "        {\"Training\": avg_loss, \"Validaton\": avg_vloss},\n",
    "        epoch + 1,\n",
    "    )\n",
    "    writer.flush()\n",
    "\n",
    "    # save best model\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = \"model_{}_{}\".format(timestamp, epoch)\n",
    "        torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def showImage(path):\n",
    "    # Load the image\n",
    "    img = Image.open(path)\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "    # Display the image in the subplot\n",
    "    axs.imshow(img)\n",
    "\n",
    "    # Disable the axis for better visualization\n",
    "    axs.axis(False)\n",
    "\n",
    "    # Set the background color of the figure as black\n",
    "    fig.set_facecolor(color=\"black\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize, Compose, ToTensor\n",
    "from vit import ViT\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize\n",
    "model = ViT()\n",
    "# Load the entire model (architecture and weights)\n",
    "model.load_state_dict(torch.load(\"predict.pth\"))\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "# Define the test_transform using Compose\n",
    "test_transform = Compose([Resize((224, 224)), ToTensor()])\n",
    "\n",
    "# Load and preprocess the test image\n",
    "path = \"../generated-images/animated-char2.jpg\"\n",
    "test_image = Image.open(path)\n",
    "test_image = test_transform(test_image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Replicate the input tensor along the batch dimension to match the expected size (32)\n",
    "batch_size = 32\n",
    "test_image_batched = test_image.repeat(batch_size, 1, 1, 1)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_image_batched)\n",
    "\n",
    "# Process predictions as needed for each image\n",
    "probabilities = F.softmax(predictions, dim=-1)\n",
    "result = \"real\" if torch.argmax(probabilities) == 0 else \"fake\"\n",
    "print(f\"Prediction: {result}\")\n",
    "showImage(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
