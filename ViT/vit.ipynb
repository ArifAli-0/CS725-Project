{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torchvision.transforms import Resize, Compose, ToTensor\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data directory\n",
    "data_dir = \"./data/Dataset\"\n",
    "\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train_transform using Compose\n",
    "train_transform = Compose([Resize((224, 224)), ToTensor()])\n",
    "\n",
    "# Define the test_transform using Compose\n",
    "test_transform = Compose([Resize((224, 224)), ToTensor()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training set\n",
    "training_dataset = ImageFolder(root=data_dir + \"/Train\", transform=train_transform)\n",
    "\n",
    "# Create the testing set\n",
    "testing_dataset = ImageFolder(root=data_dir + \"/Test\", transform=test_transform)\n",
    "\n",
    "# Create the validation set\n",
    "validation_dataset = ImageFolder(\n",
    "    root=data_dir + \"/Validation\", transform=test_transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training dataloader using DataLoader\n",
    "training_dataloader = DataLoader(\n",
    "    dataset=training_dataset, shuffle=True, batch_size=BATCH_SIZE, num_workers=12\n",
    ")\n",
    "\n",
    "# Create the testing dataloader using DataLoader\n",
    "testing_dataloader = DataLoader(\n",
    "    dataset=testing_dataset, shuffle=False, batch_size=BATCH_SIZE, num_workers=12\n",
    ")\n",
    "\n",
    "# Create the validation dataloader using DataLoader\n",
    "validation_dataloader = DataLoader(\n",
    "    dataset=validation_dataset, shuffle=False, batch_size=BATCH_SIZE, num_workers=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showRandomDataFromTraining(num_rows: int = 5):\n",
    "    num_cols = num_rows\n",
    "    # Create a figure with subplots\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(10, 10))\n",
    "\n",
    "    # Iterate over the subplots and display random images from the training dataset\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            # Choose a random index from the training dataset\n",
    "            image_index = random.randrange(len(training_dataset))\n",
    "\n",
    "            # Display the image in the subplot\n",
    "            axs[i, j].imshow(training_dataset[image_index][0].permute((1, 2, 0)))\n",
    "\n",
    "            # Set the title of the subplot as the corresponding class name\n",
    "            axs[i, j].set_title(\n",
    "                training_dataset.classes[training_dataset[image_index][1]],\n",
    "                color=\"white\",\n",
    "            )\n",
    "\n",
    "            # Disable the axis for better visualization\n",
    "            axs[i, j].axis(False)\n",
    "\n",
    "    # Set the super title of the figure\n",
    "    fig.suptitle(\n",
    "        f\"Random {num_rows * num_cols} images from the training dataset\",\n",
    "        fontsize=16,\n",
    "        color=\"white\",\n",
    "    )\n",
    "\n",
    "    # Set the background color of the figure as black\n",
    "    fig.set_facecolor(color=\"black\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 16\n",
    "IMAGE_WIDTH = 224\n",
    "IMAGE_HEIGHT = IMAGE_WIDTH\n",
    "IMAGE_CHANNELS = 3\n",
    "EMBEDDING_DIMS = IMAGE_CHANNELS * PATCH_SIZE**2\n",
    "NUM_OF_PATCHES = int((IMAGE_WIDTH * IMAGE_HEIGHT) / PATCH_SIZE**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vit Submodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddingLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        patch_size,\n",
    "        embedding_dim,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.in_channels = in_channels\n",
    "        self.conv_layer = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=embedding_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "        )\n",
    "        self.flatten_layer = nn.Flatten(start_dim=1, end_dim=2)\n",
    "        self.class_token_embeddings = nn.Parameter(\n",
    "            torch.rand((BATCH_SIZE, 1, EMBEDDING_DIMS), requires_grad=True)\n",
    "        )\n",
    "        self.position_embeddings = nn.Parameter(\n",
    "            torch.rand((1, NUM_OF_PATCHES + 1, EMBEDDING_DIMS), requires_grad=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = (\n",
    "            torch.cat(\n",
    "                (\n",
    "                    self.class_token_embeddings,\n",
    "                    self.flatten_layer(self.conv_layer(x).permute((0, 2, 3, 1))),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "            + self.position_embeddings\n",
    "        )\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, embedding_dims=768, num_heads=12, attn_dropout=0.0) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.num_head = num_heads\n",
    "        self.attn_dropout = attn_dropout\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(normalized_shape=embedding_dims)\n",
    "        self.multiheadattention = nn.MultiheadAttention(\n",
    "            num_heads=num_heads,\n",
    "            embed_dim=embedding_dims,\n",
    "            dropout=attn_dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layernorm(x)\n",
    "        output, _ = self.multiheadattention(query=x, key=x, value=x, need_weights=False)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptronBlock(nn.Module):\n",
    "    def __init__(self, embedding_dims, mlp_size, mlp_dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.mlp_size = mlp_size\n",
    "        self.dropout = mlp_dropout\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(normalized_shape=embedding_dims)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dims, out_features=mlp_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=mlp_dropout),\n",
    "            nn.Linear(in_features=mlp_size, out_features=embedding_dims),\n",
    "            nn.Dropout(p=mlp_dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(self.layernorm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dims=768,\n",
    "        mlp_dropout=0.1,\n",
    "        attn_dropout=0.0,\n",
    "        mlp_size=3072,\n",
    "        num_heads=12,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.msa_block = MultiHeadSelfAttentionBlock(\n",
    "            embedding_dims=embedding_dims,\n",
    "            num_heads=num_heads,\n",
    "            attn_dropout=attn_dropout,\n",
    "        )\n",
    "        self.mlp_block = MultiLayerPerceptronBlock(\n",
    "            embedding_dims=embedding_dims, mlp_size=mlp_size, mlp_dropout=mlp_dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.msa_block(x) + x\n",
    "        x = self.mlp_block(x) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        in_channels=3,\n",
    "        patch_size=16,\n",
    "        embedding_dims=768,\n",
    "        num_transformer_layers=6,\n",
    "        mlp_dropout=0.1,\n",
    "        attn_dropout=0.0,\n",
    "        mlp_size=3072,\n",
    "        num_heads=12,\n",
    "        num_classes=2,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embedding_layer = PatchEmbeddingLayer(\n",
    "            in_channels=in_channels, patch_size=patch_size, embedding_dim=embedding_dims\n",
    "        )\n",
    "        self.transformer_encoder = nn.Sequential(\n",
    "            *[\n",
    "                TransformerBlock(\n",
    "                    embedding_dims=embedding_dims,\n",
    "                    mlp_dropout=mlp_dropout,\n",
    "                    attn_dropout=attn_dropout,\n",
    "                    mlp_size=mlp_size,\n",
    "                    num_heads=num_heads,\n",
    "                )\n",
    "                for _ in range(num_transformer_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.classifer = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dims),\n",
    "            nn.Linear(in_features=embedding_dims, out_features=num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifer(\n",
    "            self.transformer_encoder(self.patch_embedding_layer(x))[:, 0]\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
